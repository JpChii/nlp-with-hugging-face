{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "Irrespective of profession, everyone has to wade through ocean of documents at some point to find the information for their questions.  To make matters worse, we're constantly reminded by search engines there are better ways to search! For instance the earch query \"When did Marie Curie win her first Nobel Prize?\" And google get the correct answer of \"1903\".\n",
    "\n",
    "So how was this search done? Google first retrieved 319,000 documents that were relevant to query, then performed a post processing step to extract the answer snippet with the coressponding passage and web page. But for a more trickier question like \"Which guitar tuning is best?\" We'll get web pages instead of an answer snippet.\n",
    "\n",
    "The general approach behind this technology is called *question answering*(QA), but the most common is *extractive QA* which involves questions where answers can be identified as a span in a text document, where the document might be a web page, article, legal document etc.\n",
    "\n",
    "This two stage process of retrieving relevant documents and then extracting answers from this is the basis of many modern QA systems like semantic search, intellegint assistants and automated information extractors. \n",
    "\n",
    "> **Note:** We're covering only extractive QA. There are others,\n",
    " * community QA Ex: In stack overflow, all the relevant question answer pairs will be retrived for the new Question from user and then using semantic similarity search to find the closest maching answer to this question.\n",
    " * long-form QA, which aims to generate complex paragraph-length answers to open ended questions like \"Why sky is blue?\"\n",
    " * QA can be done over tables as well, and transformer models like TAPAS can even peroform aggregations to produce the final answer!\n",
    "\n",
    "## Use Case\n",
    "\n",
    "In this notebook, we'll use QA system to solve a common problem facing ecommerce websites: helping consumers evaluate a product by answering specific queries. We'll see that the customer reviews can be used as a rich and challenging source of information for QA. Along the way we'll learn transformers capbality of *reading comprehehsion* and how the model can extract meaning from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Review-Based QA System\n",
    "\n",
    "Let's say we want to buy a smartphone, and we've question on nightmode camera capbalities. This information is not readily available in product description. We can find the answer for this in reviews by other customers. But we might've to comb throuch thousands of reviews to find the relevant answer. It'll be great if we can have a search like google right? Let's do this using transformers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "To build our QA system, we'll use [SubQA](https://arxiv.org/abs/2004.14283) dataset, which consists of 10,000+ reviews where each review is associated with a question and answer from one or more sentences from the review. The reviews are about products and services in six domains: TripAdvisor, Restaurants, Movies, Books, Electronics and Grocery.\n",
    "\n",
    "*Data example*\n",
    "\n",
    "![alt](../notes/images/7-question-answering/subqa-data-sample.png)\n",
    "\n",
    "The intersting aspect of this dataset is question(from users), answers(extraced from reviews) are *subjective*. The question on camera quality from user, the quality is subjective to each user and answer might not be from the same point of view question is raised... This is why it's easier to answer fatcual questions like \"What's the capital of a country\"\n",
    "Second, important parts of the question do not appear in review at all, so we can't use shortcuts like keyword search or paraphrasing the input question.\n",
    "\n",
    "Because of these points, SubQA is a greate dataset to benchamrk review-based QA models, sicne user-generated content in the example resembles what we might encounter in the wild.\n",
    "\n",
    "> **Note:** QA systems are categorized accordig to the domains it has access to while answering the questions. \n",
    "  * *Narrow QA* answers question about a single product less documents to search\n",
    "  * *Open-domain QA* deals with questions about almost anything(eg. amazon's whole product catalog.) more documents to search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "To build our QA system, we'll use [SubQA](https://arxiv.org/abs/2004.14283) dataset, which consists of 10,000+ reviews where each review is associated with a question and answer from one or more sentences from the review. The reviews are about products and services in six domains: TripAdvisor, Restaurants, Movies, Books, Electronics and Grocery.\n",
    "\n",
    "*Data example*\n",
    "\n",
    "![alt](../notes/images/7-question-answering/subqa-data-sample.png)\n",
    "\n",
    "The intersting aspect of this dataset is question(from users), answers(extraced from reviews) are *subjective*. The question on camera quality from user, the quality is subjective to each user and answer might not be from the same point of view question is raised... This is why it's easier to answer fatcual questions like \"What's the capital of a country\"\n",
    "Second, important parts of the question do not appear in review at all, so we can't use shortcuts like keyword search or paraphrasing the input question.\n",
    "\n",
    "Because of these points, SubQA is a greate dataset to benchamrk review-based QA models, sicne user-generated content in the example resembles what we might encounter in the wild.\n",
    "\n",
    "> **Note:** QA systems are categorized accordig to the domains it has access to while answering the questions. \n",
    "  * *Narrow QA* answers question about a single product less documents to search\n",
    "  * *Open-domain QA* deals with questions about almost anything(eg. amazon's whole product catalog.) more documents to search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "To build our QA system, we'll use [SubQA](https://arxiv.org/abs/2004.14283) dataset, which consists of 10,000+ reviews where each review is associated with a question and answer from one or more sentences from the review. The reviews are about products and services in six domains: TripAdvisor, Restaurants, Movies, Books, Electronics and Grocery.\n",
    "\n",
    "*Data example*\n",
    "\n",
    "![alt](../notes/images/7-question-answering/subqa-data-sample.png)\n",
    "\n",
    "The intersting aspect of this dataset is question(from users), answers(extraced from reviews) are *subjective*. The question on camera quality from user, the quality is subjective to each user and answer might not be from the same point of view question is raised... This is why it's easier to answer fatcual questions like \"What's the capital of a country\"\n",
    "Second, important parts of the question do not appear in review at all, so we can't use shortcuts like keyword search or paraphrasing the input question.\n",
    "\n",
    "Because of these points, SubQA is a greate dataset to benchamrk review-based QA models, sicne user-generated content in the example resembles what we might encounter in the wild.\n",
    "\n",
    "> **Note:** QA systems are categorized accordig to the domains it has access to while answering the questions. \n",
    "  * *Narrow QA* answers question about a single product less documents to search\n",
    "  * *Open-domain QA* deals with questions about almost anything(eg. amazon's whole product catalog.) more documents to search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started , let's download the dataset from hub. we can use the `get_dataset_config_names()` function to find out which subsets are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "domains = get_dataset_config_names(\"subjqa\")\n",
    "domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be focusing on developing a QA system for Electronic domain. To do that let's download the `electronics` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset subjqa (/Users/jayaprakashsivagami/.cache/huggingface/datasets/subjqa/electronics/1.1.0/2c12e496c4c675ab4a57ffb5d3f538f2e7b89793956e50da37126393ce23b6c6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bae4fd575d4c9798c2260de7479aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "subjqa = load_dataset(\n",
    "    path=\"subjqa\",\n",
    "    name=\"electronics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['domain', 'nn_mod', 'nn_asp', 'query_mod', 'query_asp', 'q_reviews_id', 'question_subj_level', 'ques_subj_score', 'is_ques_subjective', 'review_id', 'id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1295\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['domain', 'nn_mod', 'nn_asp', 'query_mod', 'query_asp', 'q_reviews_id', 'question_subj_level', 'ques_subj_score', 'is_ques_subjective', 'review_id', 'id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 358\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['domain', 'nn_mod', 'nn_asp', 'query_mod', 'query_asp', 'q_reviews_id', 'question_subj_level', 'ques_subj_score', 'is_ques_subjective', 'review_id', 'id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 255\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain\n",
      "nn_mod\n",
      "nn_asp\n",
      "query_mod\n",
      "query_asp\n",
      "q_reviews_id\n",
      "question_subj_level\n",
      "ques_subj_score\n",
      "is_ques_subjective\n",
      "review_id\n",
      "id\n",
      "title\n",
      "context\n",
      "question\n",
      "answers\n"
     ]
    }
   ],
   "source": [
    "# Features in the dataset\n",
    "for i in subjqa[\"train\"].features:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other question answering datasets on the Hub, SubjQA stores the answers to each questions as a nested dictionary. Let's inspect the answers column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bass is weak as expected', 'Bass is weak as expected, even with EQ adjusted up'], 'answer_start': [1302, 1302], 'answer_subj_level': [1, 1], 'ans_subj_score': [0.5083333253860474, 0.5083333253860474], 'is_ans_subjective': [True, True]}\n"
     ]
    }
   ],
   "source": [
    "print(subjqa[\"train\"][\"answers\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "['Bass is weak as expected', 'Bass is weak as expected, even with EQ adjusted up']\n",
      "\n",
      "answer_start\n",
      "[1302, 1302]\n",
      "\n",
      "answer_subj_level\n",
      "[1, 1]\n",
      "\n",
      "ans_subj_score\n",
      "[0.5083333253860474, 0.5083333253860474]\n",
      "\n",
      "is_ans_subjective\n",
      "[True, True]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answers has other nested keys inside\n",
    "for k, v in subjqa[\"train\"][\"answers\"][1].items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers are stored in `text` field, while the starting character indices are provided in `answer_start`. To explore the dataset more easily, let's flatten the nested dict and convert this to a `DataFrame`.\n",
    "\n",
    "We'll use dataset's [flatten()](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.flatten) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in train: 1295\n",
      "Number of questions in test: 358\n",
      "Number of questions in validation: 255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
    "for split, df in dfs.items():\n",
    "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1295 entries, 0 to 1294\n",
      "Data columns (total 19 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   domain                     1295 non-null   object \n",
      " 1   nn_mod                     1295 non-null   object \n",
      " 2   nn_asp                     1295 non-null   object \n",
      " 3   query_mod                  1295 non-null   object \n",
      " 4   query_asp                  1295 non-null   object \n",
      " 5   q_reviews_id               1295 non-null   object \n",
      " 6   question_subj_level        1295 non-null   int64  \n",
      " 7   ques_subj_score            1295 non-null   float32\n",
      " 8   is_ques_subjective         1295 non-null   bool   \n",
      " 9   review_id                  1295 non-null   object \n",
      " 10  id                         1295 non-null   object \n",
      " 11  title                      1295 non-null   object \n",
      " 12  context                    1295 non-null   object \n",
      " 13  question                   1295 non-null   object \n",
      " 14  answers.text               1295 non-null   object \n",
      " 15  answers.answer_start       1295 non-null   object \n",
      " 16  answers.answer_subj_level  1295 non-null   object \n",
      " 17  answers.ans_subj_score     1295 non-null   object \n",
      " 18  answers.is_ans_subjective  1295 non-null   object \n",
      "dtypes: bool(1), float32(1), int64(1), object(16)\n",
      "memory usage: 178.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dfs[\"train\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>domain</th>\n",
       "      <td>electronics</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_mod</th>\n",
       "      <td>great</td>\n",
       "      <td>harsh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_asp</th>\n",
       "      <td>bass response</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_mod</th>\n",
       "      <td>excellent</td>\n",
       "      <td>not strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_asp</th>\n",
       "      <td>bass</td>\n",
       "      <td>bass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_reviews_id</th>\n",
       "      <td>0514ee34b672623dff659334a25b599b</td>\n",
       "      <td>7c46670208f7bf5497480fbdbb44561a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_subj_level</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ques_subj_score</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_ques_subjective</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <td>882b1e2745a4779c8f17b3d4406b91c7</td>\n",
       "      <td>ce76793f036494eabe07b33a9a67288a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>2543d296da9766d8d17d040ecc781699</td>\n",
       "      <td>d476830bf9282e2b9033e2bb44bbb995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>B00001P4ZH</td>\n",
       "      <td>B00001P4ZH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <td>I have had Koss headphones in the past, Pro 4A...</td>\n",
       "      <td>To anyone who hasn't tried all the various typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>How is the bass?</td>\n",
       "      <td>Is this music song have a goo bass?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answers.text</th>\n",
       "      <td>[]</td>\n",
       "      <td>[Bass is weak as expected, Bass is weak as exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answers.answer_start</th>\n",
       "      <td>[]</td>\n",
       "      <td>[1302, 1302]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answers.answer_subj_level</th>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answers.ans_subj_score</th>\n",
       "      <td>[]</td>\n",
       "      <td>[0.5083333, 0.5083333]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answers.is_ans_subjective</th>\n",
       "      <td>[]</td>\n",
       "      <td>[True, True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           0  \\\n",
       "domain                                                           electronics   \n",
       "nn_mod                                                                 great   \n",
       "nn_asp                                                         bass response   \n",
       "query_mod                                                          excellent   \n",
       "query_asp                                                               bass   \n",
       "q_reviews_id                                0514ee34b672623dff659334a25b599b   \n",
       "question_subj_level                                                        5   \n",
       "ques_subj_score                                                          0.5   \n",
       "is_ques_subjective                                                     False   \n",
       "review_id                                   882b1e2745a4779c8f17b3d4406b91c7   \n",
       "id                                          2543d296da9766d8d17d040ecc781699   \n",
       "title                                                             B00001P4ZH   \n",
       "context                    I have had Koss headphones in the past, Pro 4A...   \n",
       "question                                                    How is the bass?   \n",
       "answers.text                                                              []   \n",
       "answers.answer_start                                                      []   \n",
       "answers.answer_subj_level                                                 []   \n",
       "answers.ans_subj_score                                                    []   \n",
       "answers.is_ans_subjective                                                 []   \n",
       "\n",
       "                                                                           1  \n",
       "domain                                                           electronics  \n",
       "nn_mod                                                                 harsh  \n",
       "nn_asp                                                                  high  \n",
       "query_mod                                                         not strong  \n",
       "query_asp                                                               bass  \n",
       "q_reviews_id                                7c46670208f7bf5497480fbdbb44561a  \n",
       "question_subj_level                                                        1  \n",
       "ques_subj_score                                                          0.5  \n",
       "is_ques_subjective                                                     False  \n",
       "review_id                                   ce76793f036494eabe07b33a9a67288a  \n",
       "id                                          d476830bf9282e2b9033e2bb44bbb995  \n",
       "title                                                             B00001P4ZH  \n",
       "context                    To anyone who hasn't tried all the various typ...  \n",
       "question                                 Is this music song have a goo bass?  \n",
       "answers.text               [Bass is weak as expected, Bass is weak as exp...  \n",
       "answers.answer_start                                            [1302, 1302]  \n",
       "answers.answer_subj_level                                             [1, 1]  \n",
       "answers.ans_subj_score                                [0.5083333, 0.5083333]  \n",
       "answers.is_ans_subjective                                       [True, True]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"train\"].head(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few columns, we'll use the below columns for this use case:\n",
    "\n",
    "1. `title` -> Amazon Standard Identification Number (ASIN) associated with each product\n",
    "2. `question` -> Question itself\n",
    "3. `answers.answer_text` -> The span of text in the review labelled by annotator\n",
    "4. `answers.answer_start` -> The start charachter index of the answer span\n",
    "5. `context` -> The customer review\n",
    "\n",
    "Let''s look at these columns alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>answers.text</th>\n",
       "      <th>answers.answer_start</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>B005DKZTMG</td>\n",
       "      <td>Does the keyboard lightweight?</td>\n",
       "      <td>[this keyboard is compact]</td>\n",
       "      <td>[215]</td>\n",
       "      <td>I really like this keyboard.  I give it 4 star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>B00AAIPT76</td>\n",
       "      <td>How is the battery?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>I bought this after the first spare gopro batt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                        question                answers.text  \\\n",
       "791   B005DKZTMG  Does the keyboard lightweight?  [this keyboard is compact]   \n",
       "1159  B00AAIPT76             How is the battery?                          []   \n",
       "\n",
       "     answers.answer_start                                            context  \n",
       "791                 [215]  I really like this keyboard.  I give it 4 star...  \n",
       "1159                   []  I bought this after the first spare gopro batt...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_cols = [\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "sample_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* The queries are not gramatically correct which is quite common in the FAQ sections of ecommerce websites.\n",
    "* An empty `amswers.text` entry denotes \"unanswerable\" questions whose answer cannot be found in the review.\n",
    "* We can use the start index and length of the answer span to slice out the span to get the answer from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
    "start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
    "end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this keyboard is compact'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['this keyboard is compact'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"answers.text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By predicting the start index and length of the answer, we can create a qa model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get a feel for what types of questions are available in the training set by counting the questions that starts with a few common starting words for questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What': 236,\n",
       " 'How': 515,\n",
       " 'Is': 100,\n",
       " 'Does': 45,\n",
       " 'Do': 83,\n",
       " 'Was': 12,\n",
       " 'Where': 28,\n",
       " 'Why': 21}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {}\n",
    "# Here we're assuming all users will start the question with capital letter...\n",
    "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
    "\n",
    "for q in question_types:\n",
    "    # Get the questions start with q using value_counts()\n",
    "    # value_counts() returns count for True(1st index) and False(0th index)\n",
    "    # Get true from ther value_counts() and save it to dict\n",
    "    false_true = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()\n",
    "    if len(false_true) == 2:\n",
    "        counts[q] = false_true[1]\n",
    "    else:\n",
    "        counts[q] = 0\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot this\n",
    "import matplotlib.pyplot as plt\n",
    "pd.Series(counts).sort_values().plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common questions begins with \"How\", \"What\", and \"Is\", let's have a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What\n",
      "What is direction?\n",
      "What is the quality of the construction of the bag?\n",
      "What is your impression of the product?\n",
      "\n",
      "How\n",
      "How is the camera?\n",
      "How do you like the control?\n",
      "How fast is the charger?\n",
      "\n",
      "Is\n",
      "Is this how zoom works?\n",
      "Is sound clear?\n",
      "Is it a wireless keyboard?\n",
      "\n",
      "Does\n",
      "Does the camera include control?\n",
      "Does the product have good quality?\n",
      "Does this provide enough storage?\n",
      "\n",
      "Do\n",
      "Do you have a price for a shoes?\n",
      "Does this headphone excellent?\n",
      "Do they come with the manufacturer's seal?\n",
      "\n",
      "Was\n",
      "Was the drive absolutely perfect?\n",
      "Was prompt the delivery?\n",
      "Was the mountain high?\n",
      "\n",
      "Where\n",
      "Where is the cheapest price?\n",
      "Where can I send to setup my device?\n",
      "Where do I have a drawer full of headphones?\n",
      "\n",
      "Why\n",
      "Why do I have an incredible sound?\n",
      "Why is customer service excellent?\n",
      "Why this feature affect the move?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question_type in question_types:\n",
    "    print(question_type)\n",
    "    for question in (\n",
    "        dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)].sample(n=3, random_state=42)[\"question\"]\n",
    "    ):\n",
    "        print(question)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Dataset history\n",
    "\n",
    " The (question, review, [answer setences]) format of SubjQA(used in ![7-question-answering.ipynb notebook usecase for q](../notebooks/7-question-answering.ipynb)) is commonly used in extractive QA datasets and was pioneered in Stanford Question Answering Dataset(SQuAD). \n",
    "\n",
    " * SQuAD is a famous dataset to test the abilit of machines(models) to read a passage of text and answer questions about it\n",
    " * This dataset was created from several hundred articles from wikipedia, where each of the article was partioned into paragraphs. Then crowdworkers were asked to generate questions and answers for each paragraph.\n",
    " * In the first crowdworkers version of SQuAD, answers for questions were guaranteed to be in paragraphs.\n",
    " * Soon sequence to sequence models outperformed humans and predicted spans of answers for question in the context(paragraphs, reviews etc.)\n",
    " * SQuAD 2.0 was created by augmenting SQuAD 1.1 with a set of adversial questions that were relevant to the passage but can't be answered from the passage contents alone. This became a better benchmark for evaluation qa capabalities of machines.\n",
    " * SOTA models as of 2022 with most models since 2019 surpasses human performance.\n",
    "    *qa sota models timeline*\n",
    "\n",
    "    ![alt](../notes/images/7-question-answering/qa-sota-models.png)\n",
    "* However this superhuman performance does not appear to reflect genuine reading comprehension, since answers to these \"unanswerable\" questions can be found by looking at antonymns in passages. \n",
    "* To address these problems Google released the Natural Questions (NQ) dataset which involves fact-seeking questions obtainef from Google Search users.\n",
    "* The answers in NQ are much longer than in SQuAD and present a more challenging benchmark.\n",
    "\n",
    "```\n",
    "NQ >> SQuAD2.0 >> SQuAD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dive into how transformers can extract answers from text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
