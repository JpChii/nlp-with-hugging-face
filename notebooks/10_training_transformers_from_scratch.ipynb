{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAVkdQevT5t5"
      },
      "source": [
        "# Training Transformers from Scratch\n",
        "\n",
        "In the opening paragraph of this book, we mentioned a GitHub Copilot that uses GPT like transformers to perform code autocompletion, a feature that is particulary useful when programming in a new language or framework or learning to code or for automcaticaaly producing boilerplate code.\n",
        "\n",
        "In notebook-5 we had a look at different decoding stratergies and sampling methods to generate quality text. In this notebook, we'll build our very own GPT-like model for generating Python source code! We\n",
        "ll call the resulting model *CodeParrot*.\n",
        "\n",
        "In this case we've loads of data not like multilingual ner where we've had less data for few languages and we've used transfer learning to overcome that. We'll explore the pretraining step itself and learn how to train a transformer from scratch. In this notebook, we'll cover below aspects of training which we haven't considered yet as follows,\n",
        "* Gathering and processing a very large dataset\n",
        "* Creating a custom tokenzier for our dataset\n",
        "* Training a model on multiple GPUs at scale\n",
        "\n",
        "To efficiently train large model with billions of paramters, we'll need special tools for distributed training. Although the `Trainer` from Transformers library supports distributed training, we'll take use PyTorch's Accelerate to showcase it's power/ We'll use some of the largest NLP models, but let's find a sufficiently large dataset first.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmJzkWcFqO-s"
      },
      "source": [
        "## Large Datasets and Where to Find Them\n",
        "\n",
        "There are many domains where large amount of data at hand might be available ranging from biomedical datasets to programming codebases. In mose cases, these datasets are unlabeled, and their large size means that they can usually be labeled thriugh use of heeuristics(past labelling experience) or by using accompanying metadata that is stored during the gathering process.\n",
        "\n",
        "Nevertheless unlaballed or heuristice labelled large corpus is useful. For instance it can be used to fine tune a language model for domain adaptation.\n",
        "\n",
        "The decision between fine-tuning and training from scratch is dependent on two things:\n",
        "\n",
        "1. What's the size of fine-tuning corpus?\n",
        "2. What's the domain differences between pretrained models and the corpus?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjSK9LggqPwa"
      },
      "source": [
        "When using a pretrained model it forces to use the tokenizer used with the model...\n",
        "If the tokenizer is trained on a corpus from another domain it's suboptimal.\n",
        "\n",
        "Example: using GPT's tokenizer on legal documents, other languages or even differnt sequences like musical notes or DNA sequences will result in poor tokenization.\n",
        "\n",
        "As the amount of training data we have inches closer to amount of data required for pretraing, it becomes an intersting choice to training the model and tokenizer from scratch(provided the compute resources).\n",
        "\n",
        "Before we discuss the pretrainig objectives, we'll have to build a large corpus which comes with it's own challenges. Let's explore that next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V4oFwgmNKEI",
        "outputId": "c4db03df-14cb-4c34-c3de-c73d74dc2cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOARqMbLqQPo"
      },
      "source": [
        "### Challenges of Building a Large-Scale Corpus\n",
        "\n",
        "Quality of a pretrained model depends on the pretrained corpus itself, as the model inherits defects from the corpus. Hence before creating one, let's become aware of the common issues and challenges associated with building a large corpora for pretraining.\n",
        "\n",
        "```\n",
        "Pretraining Corpus[good/bad] ---Training---> Pretrained model[good/bad]\n",
        "```\n",
        "\n",
        "***1. Can we be aware of what's inside a very large dataset?***\n",
        "\n",
        "*As the dataset grows larger and larger, the chances of full control or precise idea of what is inside dimishes.*\n",
        "\n",
        "***2. How is a large dataset created? which might give some information on visiblity of the dataset***\n",
        "\n",
        "* *It's not created by dedicated people who create one sample at a time, while being aware and knowledgeable of the full pipeline and the task that the machine learning model will be applied to.*\n",
        "* *It has more chances of creating in an automatic or semiautomatic way by collecting data that is a side effect of some other activites. For example, it may consists of all the documents(contracts, purchase orders etc.) that a company stores, logs from user activites, or data gatherd from internet.*\n",
        "\n",
        "***3. What are the consequences of creating a corpora with such high degree of automation?***\n",
        "\n",
        "* *Limited control over the content and te way ther are created, thus increasing the risk of training a model on biased or lower-quality data.*\n",
        "* *Recent investigations of large-scale datasets like BookCorpus and C4 which were used to train BERT and T4, have uncoverd (among other things) that:*\n",
        "    * A significant proportion of the C4 corpus is machine-translated rather than by humans.\n",
        "    * Disparate erasure of African-American English as a result of stopword filtering in C4 resulted in an underrepresentation of such content.\n",
        "    * It's typically diffult to find a middle ground between including(often too much) sexually or other explicit content and totally ersation all mention of sexuality or gender. As a surprising consequence of this, a rather common word like sex(both neutral and explicit meanings) is completley unknown to tokenizer that is trained on C4, since this word is absent form corpus.\n",
        "\n",
        "This discrepancies might not be incompatible if the downstream task requries such a skew. For example, In BookCorpus there's a strong overepresentation of romance novels and if a model is intended to be romance novel writing tool this skew is good for this task.\n",
        "\n",
        "Let's checkout this skew on model based on the dataset by comparing GPT and GPT-2 models with same propmt, similar-sized versions where only datasets differ. BookCorpus(GPT) vs Webpages, blogs and new articles linked from reddit.\n",
        "\n",
        "By using `text-generation` pipeline to investigate the model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQebMGIhqQUo"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
        "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs_zCs-3qQdh",
        "outputId": "5529d8e5-49c9-4ffc-8d15-99437702da1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT Size: 116.5M parameters\n",
            "GPT2 size: 124.4M parameters\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate total number of paramters in the model\n",
        "def model_size(model):\n",
        "  return sum(param.numel() for param in model.parameters())\n",
        "\n",
        "print(f\"GPT Size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
        "print(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3K_hROsqQf_"
      },
      "source": [
        "We're using the original gpt model vs smallest gpt 2 model and they have the same number of parameters. Next let's generate three different completions from each model, with the same input prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUrqTmZlqQjW"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "def enum_pipeline_outputs(\n",
        "    pipe: transformers.pipeline,\n",
        "    prompt: str,\n",
        "    num_return_sequences: int\n",
        "    ) -> str:\n",
        "  \"\"\"\n",
        "  Function to generate text using text-generation pipeline\n",
        "\n",
        "  Args:\n",
        "    pipe (transformers.pipeline): Text generation pipeline to use to generate text\n",
        "    prompt (str): Input text prompt to genreate text\n",
        "    num_return_sequences (int): Number of sequences to generate\n",
        "\n",
        "  Returns:\n",
        "    str: Returns sequences generated\n",
        "  \"\"\"\n",
        "\n",
        "  out = pipe(\n",
        "      prompt,\n",
        "      num_return_sequences=num_return_sequences,\n",
        "      clean_up_tokenization_spaces=True,\n",
        "      )\n",
        "  return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUNDBFa1qQmz",
        "outputId": "dfa099a6-4fc2-4ca7-8c88-4c52185675d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT completions: \n",
            " 1.\n",
            "When they came back, the next one would be the best. \n",
            " the first and only person who might pped a gun was david becker. \n",
            " becker stared up at the gray beams from the ceiling. he was terrified. the light was blinding. it\n",
            "2.\n",
            "When they came back, she was ready. \" \n",
            " my jaw had dropped. \" you've been watching me? \" \n",
            " \" uh - huh, \" he smiled. \" i found your address on the phone. \" \n",
            " that made sense. i\n",
            "3.\n",
            "When they came back. \n",
            " i would find out soon enough. but right now, my mind was busy processing all the information i was learning to deal with at a late stage of the journey in an uncomfortable sort of way. once we were in the\n",
            "GPT-2 completions: \n",
            " 1.\n",
            "When they came back to look over their shoulders they noticed a tiny black bear and she ran away; I thought they'd be worried, though the bear had a very unusual, large, tail with a large sharp sharp-edged claw.\n",
            "The\n",
            "2.\n",
            "When they came back to him at the top of the stairs.\n",
            "\n",
            "\"I want to say this to anyone in the world out there who's afraid of Muslims,\" Sahlik said. \"Just stop telling me what I don't want\n",
            "3.\n",
            "When they came back to work, they said, \"I can't do anything about that. This is the best that's possible for me, so I'm sure we'll do something for next year.\" They also had some ideas. \"I\n"
          ]
        }
      ],
      "source": [
        "# Let's generate some text using the function on above cell\n",
        "prompt = \"\\nWhen they came back\"\n",
        "gpt_completions = enum_pipeline_outputs(\n",
        "    pipe=generation_gpt,\n",
        "    prompt=prompt,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "gpt_2_completions = enum_pipeline_outputs(\n",
        "    pipe=generation_gpt2,\n",
        "    prompt=prompt,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "print(f\"GPT completions: \\n {gpt_completions}\")\n",
        "print(f\"GPT-2 completions: \\n {gpt_2_completions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XmxgMuOqQqV"
      },
      "source": [
        "On looking at these few samples, we can see the romantic skew in GPT generation, which will typically imagine an interaction between a man and a woman. On the other hand GPT-2 generation trained on webtext linked to and from reddit articles and mostly adopts a neutral *they* in it's generationsm whicg contation \"blog-like\" or adventure related elemets.\n",
        "\n",
        "In general, any model trained on dataset will reflect the language bias and over-or underrepresentation of populations and events in its training data. These biases in the behaviour of the model are importatnt to take into consideration with reagard to the target audience interacting with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WchIbJLeqQsj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX62iDmdqQwY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWPgnvQQqQzN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxnUMIrTqQ10"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI8kfNvVqmaO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
