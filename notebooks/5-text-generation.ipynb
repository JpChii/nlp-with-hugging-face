{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omlca8Az_zFd"
      },
      "source": [
        "# Text-Generation\n",
        "\n",
        "Transformer language models has an uncanny feature of generating text that is almost indistinguishable from human text. This text generation happens without any explicit supervised leraning, just by predicting the next word based on context in a millions of web pages. With just pretraining LLM's learn a special set of skills and pattern recognition abilites that can be activated with different kind of prompts.\n",
        "\n",
        "![pretraining-sequence-of-tasks](https://github.com/JpChii/nlp-with-hugging-face/blob/main/notes/images/5-text-generation/pretraining-model-sequence-of-tasks.png?raw=1)\n",
        "\n",
        "The image shows addition, unscramling, translation are some of the sequence tasks that an LLM is exposed during training. This knowledge is transferred during fine-tuning(for larger models during inference-time). These tasks are not chosen specifically ahead of time and occur naturally with huge corpora.\n",
        "\n",
        "With the advent of GPT-4 and now an open sourced LLAMA2, has given rise to lot's of applications with LLM's at its core with text generation capacity.\n",
        "\n",
        "In [5-text-generation.ipynb](../notebooks/5-text-generation.ipynb) notebook we'\n",
        "ll cover how text generation works with LLM's and how different decoding stratergies impact text generation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.21.0 transformers==4.31.0 datasets==2.14.2"
      ],
      "metadata": {
        "id": "I6yqwlkHAB_y",
        "outputId": "d670e7dc-b2ca-4514-fa75-f21bb66e7b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: datasets==2.14.2 in /usr/local/lib/python3.10/dist-packages (2.14.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.2) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.2) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.2) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.2) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.2) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.2) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.2) (3.8.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.2) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.2) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.2) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.2) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.2) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.2) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.2) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (16.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.2) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.2) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDw4JP7H_zFg"
      },
      "source": [
        "## The Challenge with Generating coherente Text\n",
        "\n",
        "Until now in the series of notebook, we used a body and a fine-tuned head to get logits. Then we use argmax on logits to get a predicted class or softmax to get prediction probabalites for each token. By contrast, converting the model's probablistic output to text requries a *decoding method*, which introduces a few challenges unique to text generation:\n",
        "\n",
        "* The decoding is done *iteratively* and requires more compure, not like passing the inputs through forward pass just once.\n",
        "* The *quality* and *diversity* of text generated depends on the decoding method and associated hyperparameters.\n",
        "\n",
        "To understand how this decoding process works, let's start by examining how GPT-2 is pretrained and subsequently applied to genreate text.\n",
        "\n",
        "Like other *autoregressive* or *casual language models* GPT-2 is pretrained to estimate the probabality p(X|Y) of a sequence of tokens **y** = y1, y2,...yt, given some initial context **x** = x1, x2,...xt. Since it's impossible to acquire enough training data, the chain rule of proabality is used to factorize it as a product of *conditional probabalities*.\n",
        "\n",
        "*Predicting token c given a and b are before it is the conditional probablity intutition*.\n",
        "\n",
        "![alt contitional-proabablity](https://github.com/JpChii/nlp-with-hugging-face/blob/main/notes/images/5-text-generation/llm-product-of-conditional-probabalities.png?raw=1)\n",
        "\n",
        "The note above describe exactly the probablity calculation on right side. This pretraining objective is quite different from BERT's, which utilizer both past and furture contexts to predict a masked token.\n",
        "\n",
        "We can generate a text by predicting next token, adding it to the sequence and use this as new sequenct to predict next token and continue this iterative process until a special end of sequence token.\n",
        "\n",
        "Example of this process below,\n",
        "![text-generation](https://github.com/JpChii/nlp-with-hugging-face/blob/main/notes/images/5-text-generation/text-generation.png?raw=1)\n",
        "\n",
        "> **Note:** Since the output sequence is *conditioned* on the choice of input prompt, this type of text genreation is often called as *conditional text generation*.\n",
        "\n",
        "At the heart of this process lies the decoding method that determines which token is selected at each time step.\n",
        "\n",
        "A language model produces a logit for each word in  the vocabulary at each time step, we can get the probabality distribution for each token using softmax.\n",
        "\n",
        "![next-token-softmax](https://github.com/JpChii/nlp-with-hugging-face/blob/main/notes/images/5-text-generation/next-token-softmax.png?raw=1)\n",
        "\n",
        "The goal of most decoding methods is to search for the most likelt overall sequence by picking a y_hat such that:\n",
        "\n",
        "![next-token-softmax](https://github.com/JpChii/nlp-with-hugging-face/blob/main/notes/images/5-text-generation/next-token-argmax.png?raw=1)\n",
        "\n",
        "\n",
        "Finding y_hat directly involve evaluating every possible sequence with the language model. Since there does not exist an algorithm to do this within an reasonable amount of time we use approximation instead. In this note, we'll explore few of these approximation methods and gradullay build up toward smarter and more complex algorithms that can gernerate high quality texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HvA4y3S_zFl"
      },
      "source": [
        "## Greedy Search Decoding\n",
        "\n",
        "The simplest decoding method to get discrete tokens from a model's continuous output is to greedily select the token with the highest probabality at each timestep.\n",
        "\n",
        "*Greedy search decoding argmax*\n",
        "![alt](https://github.com/JpChii/nlp-with-hugging-face/blob/main/notes/images/5-text-generation/greedy-search-decoding.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EBpnWat_zFm"
      },
      "source": [
        "To see how greedy search works, let's load a 1.5 billion-parameter version of GPT-2 with a language modelling head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W29gZKqQ_zFn"
      },
      "outputs": [],
      "source": [
        "from accelerate import init_empty_weights\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpHbk77f_zFr"
      },
      "source": [
        "Now let's generate some text! Although Transformers provides a generate() function for autoregressive models like GPT-2, we'll implement this decoding method to understand what's going on under the hood."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "O-ytYfT8GUHk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "input_txt = \"Transformers are the\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "# List to store dicts of input context and next top 5 probabale tokens\n",
        "iterations = []\n",
        "# Number of steps to generate tokens\n",
        "n_steps = 8\n",
        "# Number of choices\n",
        "choices_per_step = 5\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Loop to generate tokens for n_steps\n",
        "  for _ in range(n_steps):\n",
        "    iteration = dict()\n",
        "    iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
        "\n",
        "    # Get model outputs\n",
        "    outputs = model(input_ids=input_ids)\n",
        "\n",
        "    # Logits --> probs -->\n",
        "    next_token_logits = outputs.logits[0, -1, :] # Get logits for last token(-1) in the first batch(0)\n",
        "    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "    sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
        "\n",
        "    for choice_idx in range(choices_per_step):\n",
        "      token_id = sorted_ids[choice_idx]\n",
        "      token_prob = next_token_probs[token_id].cpu().numpy()\n",
        "      token_choice = {\n",
        "          f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
        "      }\n",
        "      iteration[f\"Choice {choice_idx}\"] = token_choice\n",
        "\n",
        "    input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
        "    iterations.append(iteration)\n",
        "\n",
        "pd.DataFrame(iterations)"
      ],
      "metadata": {
        "id": "YdLiRLX6_6gS",
        "outputId": "5540c95a-7905-4d48-9913-938a97fca5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Input              Choice 0  \\\n",
              "0                               Transformers are the       { most (8.37%)}   \n",
              "1                          Transformers are the most  { powerful (20.77%)}   \n",
              "2                 Transformers are the most powerful     { beings (9.43%)}   \n",
              "3          Transformers are the most powerful beings        { in (56.16%)}   \n",
              "4       Transformers are the most powerful beings in       { the (72.89%)}   \n",
              "5   Transformers are the most powerful beings in the  { universe (67.94%)}   \n",
              "6  Transformers are the most powerful beings in t...          {. (35.28%)}   \n",
              "7  Transformers are the most powerful beings in t...      { They (32.09%)}   \n",
              "\n",
              "                Choice 1            Choice 2                 Choice 3  \\\n",
              "0        { only (3.35%)}     { best (2.75%)}         { first (2.54%)}   \n",
              "1      { common (7.09%)}  { popular (5.09%)}     { important (3.29%)}   \n",
              "2         { and (8.35%)}       { of (4.61%)}  { Transformers (4.34%)}   \n",
              "3         { on (18.99%)}    { known (3.12%)}            { of (3.09%)}   \n",
              "4  { existence (11.20%)}      { all (3.40%)}      { creation (1.81%)}   \n",
              "5    { Universe (5.41%)}   { Marvel (4.40%)}  { Transformers (3.49%)}   \n",
              "6           {, (34.16%)}     { and (12.94%)}              {; (1.55%)}   \n",
              "7           {\\n (4.97%)}    { Their (4.93%)}           { The (3.85%)}   \n",
              "\n",
              "                  Choice 4  \n",
              "0      { ultimate (2.20%)}  \n",
              "1      { advanced (2.72%)}  \n",
              "2              {, (3.83%)}  \n",
              "3            { to (2.18%)}  \n",
              "4  { Transformers (1.18%)}  \n",
              "5          { mult (3.47%)}  \n",
              "6              {! (1.34%)}  \n",
              "7           { But (2.88%)}  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-d6dfa110-64c0-42be-8df9-108481b90347\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Choice 0</th>\n",
              "      <th>Choice 1</th>\n",
              "      <th>Choice 2</th>\n",
              "      <th>Choice 3</th>\n",
              "      <th>Choice 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Transformers are the</td>\n",
              "      <td>{ most (8.37%)}</td>\n",
              "      <td>{ only (3.35%)}</td>\n",
              "      <td>{ best (2.75%)}</td>\n",
              "      <td>{ first (2.54%)}</td>\n",
              "      <td>{ ultimate (2.20%)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Transformers are the most</td>\n",
              "      <td>{ powerful (20.77%)}</td>\n",
              "      <td>{ common (7.09%)}</td>\n",
              "      <td>{ popular (5.09%)}</td>\n",
              "      <td>{ important (3.29%)}</td>\n",
              "      <td>{ advanced (2.72%)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Transformers are the most powerful</td>\n",
              "      <td>{ beings (9.43%)}</td>\n",
              "      <td>{ and (8.35%)}</td>\n",
              "      <td>{ of (4.61%)}</td>\n",
              "      <td>{ Transformers (4.34%)}</td>\n",
              "      <td>{, (3.83%)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transformers are the most powerful beings</td>\n",
              "      <td>{ in (56.16%)}</td>\n",
              "      <td>{ on (18.99%)}</td>\n",
              "      <td>{ known (3.12%)}</td>\n",
              "      <td>{ of (3.09%)}</td>\n",
              "      <td>{ to (2.18%)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Transformers are the most powerful beings in</td>\n",
              "      <td>{ the (72.89%)}</td>\n",
              "      <td>{ existence (11.20%)}</td>\n",
              "      <td>{ all (3.40%)}</td>\n",
              "      <td>{ creation (1.81%)}</td>\n",
              "      <td>{ Transformers (1.18%)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Transformers are the most powerful beings in the</td>\n",
              "      <td>{ universe (67.94%)}</td>\n",
              "      <td>{ Universe (5.41%)}</td>\n",
              "      <td>{ Marvel (4.40%)}</td>\n",
              "      <td>{ Transformers (3.49%)}</td>\n",
              "      <td>{ mult (3.47%)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Transformers are the most powerful beings in t...</td>\n",
              "      <td>{. (35.28%)}</td>\n",
              "      <td>{, (34.16%)}</td>\n",
              "      <td>{ and (12.94%)}</td>\n",
              "      <td>{; (1.55%)}</td>\n",
              "      <td>{! (1.34%)}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Transformers are the most powerful beings in t...</td>\n",
              "      <td>{ They (32.09%)}</td>\n",
              "      <td>{\\n (4.97%)}</td>\n",
              "      <td>{ Their (4.93%)}</td>\n",
              "      <td>{ The (3.85%)}</td>\n",
              "      <td>{ But (2.88%)}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6dfa110-64c0-42be-8df9-108481b90347')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-0f282707-3aa8-4650-8502-ad054851be35\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f282707-3aa8-4650-8502-ad054851be35')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-0f282707-3aa8-4650-8502-ad054851be35 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6dfa110-64c0-42be-8df9-108481b90347 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6dfa110-64c0-42be-8df9-108481b90347');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this simple method we were able to generate the sentence \"Transformers are the most powerful beings in the universe\". Interestingly this indicates that GPT-2 has internalized some knowledge about the media franchise, which was created by two companies(Hasbro and Takara Tony).\n",
        "\n",
        "We can also see other possible continuations at each step, highlighting the iterative nature of text generation. Unlike in sequence classification tasks where a single forward pass suffices to generate the predictrions, with text generation we need to decode the output tokens one at a time.\n",
        "\n",
        "Let's try out the transformers `generate()` function to explore more sophisticated decoding startegies."
      ],
      "metadata": {
        "id": "3tMCRx6BGRDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens = tokenizer(\n",
        "    input_txt,\n",
        "    return_tensors=\"pt\"\n",
        ")[\"input_ids\"].to(device)\n",
        "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "KvvcZ22tIQzU",
        "outputId": "7731b9b8-eb4a-430e-cc6d-45908047526d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers are the most powerful beings in the universe. They are the creators of the universe, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, let's try out the OpenAI's unicrorn story. We'll encode the prompt using tokenizer and increase the `max_legth` to generate a longer text."
      ],
      "metadata": {
        "id": "llGHJoQjIjkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
        "a herd of unicorns living in a remote, previously unexplored \\\n",
        "valley, in the Andes Mountains. Even more surprising to the \\\n",
        "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
        "\"\"\"\n",
        "input_ids = tokenizer(\n",
        "    input_txt,\n",
        "    return_tensors=\"pt\"\n",
        ")[\"input_ids\"].to(device)\n",
        "output = model.generate(\n",
        "    input_ids, # Input encoded ids from tokens\n",
        "    max_new_tokens=max_length, # Number of tokens to generate\n",
        "    do_sample=False\n",
        ")\n",
        "output.shape"
      ],
      "metadata": {
        "id": "7TNC9uWMue2d",
        "outputId": "112cdd3a-df5b-42b2-9327-003c02215e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 175])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "tlwUkijeu0xI",
        "outputId": "744183a5-7942-46fb-d358-651e726d496e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "\n",
            "The researchers, led by Dr. David M. Koehler, a professor of anthropology at the University of Colorado, Boulder, discovered the unicorns in the remote valley of La Paz, in the Andes Mountains.\n",
            "\n",
            "\n",
            "\"We were surprised to find that the unicorns spoke perfect English,\" said Koehler. \"They were very friendly and friendly with us. They were very friendly with us and we were very friendly with them.\"\n",
            "\n",
            "\n",
            "The researchers were able to identify the unicorns by their distinctive pattern of white spots on their backs. The researchers also found that the unicorns were able to communicate with each other.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see a repeat in sequences, *They were very friendly with us* and *The researchers* were repeated twice. This is because how greedy search works.\n",
        "\n",
        "It selects the token based on the highest probabality token withut considering the sequence. To generate good sequences it'll be better to select tokens based on the overall sequence probabality."
      ],
      "metadata": {
        "id": "tc4LKtsvu7Fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy search decoding is not preferred to generate diverse text but it's useful for where a deterministic and factually correct output is required like arithmetic use cases"
      ],
      "metadata": {
        "id": "JXfRfI9svsKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation for arithmetic use case using greedy searc\n",
        "max_length = 5\n",
        "input_text = \"\"\"\n",
        "5 + 8 => 13 \\\n",
        "7 + 1 => 8 \\\n",
        "1 + 1 =>\n",
        "\"\"\"\n",
        "input_ids = tokenizer(\n",
        "    input_txt,\n",
        "    return_tensors=\"pt\",\n",
        ")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(\n",
        "    input_ids,\n",
        "    max_length=max_length,\n",
        "    do_sample=False,\n",
        ")\n",
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "id": "VVxFQWp-wCAJ",
        "outputId": "2dfbddde-7266-42d8-c977-9da018e6b37d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Input length of input_ids is 47, but `max_length` is set to 5. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "\n",
            "The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generation worked correctly in gpt-large checkpoint, not loading and testing it out here due to colab memory constraints."
      ],
      "metadata": {
        "id": "kVnz-YF3wgky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam Search Decoding\n",
        "\n",
        "Instead of decoding with the next highest probable token, beam search decoding keeps track of the *top-b* most probable next tokens, where *b* is refrred to as *number of beam* or *partial hypotheses. Then the next set of beams will be chose based on the current beams. This process is repeated until eos token or max_length. Finally the sequence is selected based on ranking the *b* beams accordig to log probabalities or the beam path of a sequence with highest probabality.\n",
        "\n",
        "![beam-search-decoding-with-two-beams](https://github.com/JpChii/nlp-with-hugging-face/blob/main/notes/images/5-text-generation/beam-search-with-two-beams.png?raw=1)\n",
        "\n",
        "*How are the sequences scored?*\n",
        "\n",
        "We'll score the sequences using log probabaites instead of probabality itself. Because calculating the probablity of a sequence requires product of condition probabalites which is between [0,1].\n",
        "\n",
        "Assume we've 1024 tokens with 0.5 conditional probablity the product will be so small that the computer won't be able to represent the result and cause underflow. This extremeley small number also leads to numerical unstability with the underflow. We can avoid this by applying a log on top of conditional probabalites before the product and sum of all of them. This'll reduce the risk of running into numerical unstability."
      ],
      "metadata": {
        "id": "tard-bOOxP7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Without log ranking a sequence\n",
        "0.5 ** 1024"
      ],
      "metadata": {
        "id": "amumDbbByQeU",
        "outputId": "a3cc84d5-8e15-44ff-fcab-ffb657a4d5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.562684646268003e-309"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With log\n",
        "np.log(0.5) ** 1024"
      ],
      "metadata": {
        "id": "DlUhuykg3e8v",
        "outputId": "da7a30c8-639f-457d-aee0-da2c78c0ed32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0122134649886695e-163"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(\n",
        "    [np.log(0.5)] * 1024\n",
        ")"
      ],
      "metadata": {
        "id": "iUT_vT513jB8",
        "outputId": "d2645fd9-4e60-4dfa-bb54-858084f5e7ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-709.782712893384"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This aproach works for much smaller numbers as well. Since we need to compare the relative probabalities, we can do this directly with log probabalities."
      ],
      "metadata": {
        "id": "kqhRPM773oJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## calculating log probabalities of sequences\n",
        "\n",
        "Let's calculate and compare the log probabalities of the texts generated by greedy and beam search to see if beam search can improve the overall probabality.\n",
        "\n",
        "Input tokens --> transformers models --> Unnormalized logits for the next token.\n",
        "\n",
        "First we've to nromalize the logits to create a probabality distribution over the whole vocabulary for each token in the sequence. Then select only the token probabalities that were present in the sequence. The following functions implement the logic.\n",
        "\n",
        "> Why probabaility over entire vocabulary for logits from sequence alone? Think of it like this, the next token can be any word from the vocabulary right? so taking the probabality over all the possible token is the correct approach to get the correct probabality for tokens in sequence."
      ],
      "metadata": {
        "id": "jFRPHOVh4FJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_greedy.shape"
      ],
      "metadata": {
        "id": "OwhTouWo-3LY",
        "outputId": "016f9a65-c5fa-4cc5-e664-e264a581ee1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 48])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_greedy.unsqueeze(2).shape"
      ],
      "metadata": {
        "id": "J9EBMP1mAlW5",
        "outputId": "6da56696-e05b-474b-850f-4969168e8634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 48, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_greedy.unsqueeze(2).unsqueeze(-1).shape"
      ],
      "metadata": {
        "id": "IuPcr-C2BOUc",
        "outputId": "e0959252-1c8d-4eaf-f9c1-d291e81938b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 48, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_greedy.unsqueeze(2).squeeze(-1).shape"
      ],
      "metadata": {
        "id": "sfEGki8vBUbM",
        "outputId": "96933d35-191f-4271-cd2a-e2e5a31a71af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 48])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_greedy.unsqueeze(2).squeeze(-1), output_greedy"
      ],
      "metadata": {
        "id": "YWuKJNGoFeV5",
        "outputId": "6a838eb2-22bf-436f-cedf-68703c78f9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
              "          28000, 19942,  2877,   287,   257,  6569,    11,  4271, 31286,  1850,\n",
              "          19272,    11,   287,   262,   843,   274, 21124,    13,  3412,   517,\n",
              "           6452,   284,   262,  4837,   373,   262,  1109,   326,   262, 28000,\n",
              "          19942,  5158,  2818,  3594,    13,   628,   198,   464]]),\n",
              " tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
              "          28000, 19942,  2877,   287,   257,  6569,    11,  4271, 31286,  1850,\n",
              "          19272,    11,   287,   262,   843,   274, 21124,    13,  3412,   517,\n",
              "           6452,   284,   262,  4837,   373,   262,  1109,   326,   262, 28000,\n",
              "          19942,  5158,  2818,  3594,    13,   628,   198,   464]]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(output_greedy)"
      ],
      "metadata": {
        "id": "8NazrIV5BXGE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.keys()"
      ],
      "metadata": {
        "id": "6X_UwP7KBfZ-",
        "outputId": "9c81afc4-ffc5-46b1-9858-ad10ba14b749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['logits', 'past_key_values'])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_greedy[:, 1:]"
      ],
      "metadata": {
        "id": "F_0e1aZyBwHp",
        "outputId": "d7c72f89-a229-4f43-9f8b-accd9958c99d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286, 28000,\n",
              "         19942,  2877,   287,   257,  6569,    11,  4271, 31286,  1850, 19272,\n",
              "            11,   287,   262,   843,   274, 21124,    13,  3412,   517,  6452,\n",
              "           284,   262,  4837,   373,   262,  1109,   326,   262, 28000, 19942,\n",
              "          5158,  2818,  3594,    13,   628,   198,   464]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_greedy"
      ],
      "metadata": {
        "id": "SD1Ox9QWDDJL",
        "outputId": "1fa59cbd-15de-4567-b890-3928cd5f7201",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
              "         28000, 19942,  2877,   287,   257,  6569,    11,  4271, 31286,  1850,\n",
              "         19272,    11,   287,   262,   843,   274, 21124,    13,  3412,   517,\n",
              "          6452,   284,   262,  4837,   373,   262,  1109,   326,   262, 28000,\n",
              "         19942,  5158,  2818,  3594,    13,   628,   198,   464]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate log probabality for a single token\n"
      ],
      "metadata": {
        "id": "fWIvlcLzEq0h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}