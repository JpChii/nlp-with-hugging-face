# Training transformers from scratch

* 📝To train large model with billions of paramters, we need special tools for distributed training... Training a model of this capacity requried multiple GPU's'

📝Choosing between training or fine-tuning depends on two things?

1. Corpus size
    * As the amount of training data we have inches closer to amount of data required for pretraing, it becomes an intersting choice to training the model and tokenizer from scratch(provided the compute resources).
2. Difference between corpus and pretrained model corpus
    * ⚠️ Pretrained model forces us to use it's tokenizer. If the tokenizer is trained on a different domain then the results will be poor, as tokenization is the inital step of converting raw text to numbers.

## Large Datasets and Where to Find Them

There are many domains where large amount of data at hand might be available ranging from biomedical datasets to programming codebases. In mose cases, these datasets are unlabeled, and their large size means that they can usually be labeled thriugh use of heeuristics(past labelling experience) or by using accompanying metadata that is stored during the gathering process.

Nevertheless unlaballed or heuristice labelled large corpus is useful. For instance it can be used to fine tune a language model for domain adaptation.

The decision between fine-tuning and training from scratch is dependent on two things:

1. What's the size of fine-tuning corpus?
2. What's the domain differences between pretrained models and the corpus?

### Challenges of Building a Large-Scale Corpus

Quality of a pretrained model depends on the pretrained corpus itself, as the model inherits defects from the corpus. Hence before creating one, let's become aware of the common issues and challenges associated with building a large corpora for pretraining.

```
Pretraining Corpus[good/bad] ---Training---> Pretrained model[good/bad]
```

***1. Can we be aware of what's inside a very large dataset?***

*As the dataset grows larger and larger, the chances of full control or precise idea of what is inside dimishes.*

***2. How is a large dataset created? which might give some information on visiblity of the dataset***

* *It's not created by dedicated people who create one sample at a time, while being aware and knowledgeable of the full pipeline and the task that the machine learning model will be applied to.*
* *It has more chances of creating in an automatic or semiautomatic way by collecting data that is a side effect of some other activites. For example, it may consists of all the documents(contracts, purchase orders etc.) that a company stores, logs from user activites, or data gatherd from internet.*

***3. What are the consequences of creating a corpora with such high degree of automation?***

* *Limited control over the content and te way ther are created, thus increasing the risk of training a model on biased or lower-quality data.*
* *Recent investigations of large-scale datasets like BookCorpus and C4 which were used to train BERT and T4, have uncoverd (among other things) that:*
    * A significant proportion of the C4 corpus is machine-translated rather than by humans.
    * Disparate erasure of African-American English as a result of stopword filtering in C4 resulted in an underrepresentation of such content.
    * It's typically diffult to find a middle ground between including(often too much) sexually or other explicit content and totally ersation all mention of sexuality or gender. As a surprising consequence of this, a rather common word like sex(both neutral and explicit meanings) is completley unknown to tokenizer that is trained on C4, since this word is absent form corpus.

This discrepancies might not be incompatible if the downstream task requries such a skew. For example, In BookCorpus there's a strong overepresentation of romance novels and if a model is intended to be romance novel writing tool this skew is good for this task.

***Some gpt completions to validate the skew above***

*The below text are generated by two different gpt models of same size on same input prompt but different dataset, gpt-2 is trained on web and blogs reddit articles.*

```
GPT completions:
 1.
When they came back, the next one would be the best.
 the first and only person who might pped a gun was david becker.
 becker stared up at the gray beams from the ceiling. he was terrified. the light was blinding. it
2.
When they came back, she was ready. "
 my jaw had dropped. " you've been watching me? "
 " uh - huh, " he smiled. " i found your address on the phone. "
 that made sense. i
3.
When they came back.
 i would find out soon enough. but right now, my mind was busy processing all the information i was learning to deal with at a late stage of the journey in an uncomfortable sort of way. once we were in the
GPT-2 completions:
 1.
When they came back to look over their shoulders they noticed a tiny black bear and she ran away; I thought they'd be worried, though the bear had a very unusual, large, tail with a large sharp sharp-edged claw.
The
2.
When they came back to him at the top of the stairs.

"I want to say this to anyone in the world out there who's afraid of Muslims," Sahlik said. "Just stop telling me what I don't want
3.
When they came back to work, they said, "I can't do anything about that. This is the best that's possible for me, so I'm sure we'll do something for next year." They also had some ideas. "I
```

On looking at these few samples, we can see the romantic skew in GPT generation, which will typically imagine an interaction between a man and a woman. On the other hand GPT-2 generation trained on webtext linked to and from reddit articles and mostly adopts a neutral *they* in it's generationsm whicg contation "blog-like" or adventure related elemets.

In general, any model trained on dataset will reflect the language bias and over-or underrepresentation of populations and events in its training data. These biases in the behaviour of the model are importatnt to take into consideration with reagard to the target audience interacting with the model.

The brief introduction above will give an idea of the difficult challenges faced during creation of a large corpora. With these in mind, let's create a large dataset.

## Building a Custom Code Dataset

Refer page 681 to 686 to create a custom dataset using below google bigquery.

```Sql
SELECT f.repo_name, f.path, c.copies, c.size, c.content
FROM `bigquery-public-data.github_repos.files` AS f
JOIN `bigquery-public-data.github_repos.contents` AS c ON f.id = c.id
JOIN `bigquery-public-data.github_repos.licenses` AS l ON f.repo_name = l.repo_name
WHERE  NOT c.binary
 AND ((f.path LIKE '%.py')
 AND (c.size BETWEEN 1024
 AND 1048575))
```

* In this section, the results above query is exported to Google bucket and downloaded with gsutil to local.
* BigQuery gives access to google inventory, which has snapshots of github repos to allow us to download data in bulk without limitations of Github REST API.
* The query creates an export of public python repositores in github
* The results of this query is around 2.7TB uncompressed data and 50GB of compressed data.

***To filter the noise or not?***
* We've to make decisions on the noise in the data, 
    * Noise in training data can make system more robust to noisy inputs at inference time
    * Will also make random predictions 
* Depending on intended use and whole system integration, we can choose between more or less noisy data and add pre and post filtering operations.
* Data preparation is a crucial step and we've to clean up the dataset as much as possible.

### Working with large Datasets

Loading a large dataset of 50GB compressed data or 200TB of compressed data into RAM is a challenging task.
`Datasets` is designed in a way to overcome this with two specific features that allows to free from RAM and hard drive space limitations: memeory mapping and streaming.

#### Memory mapping

* Dataset activate zero-copy and zero-overhead memory by default.
* A direct reflection of content in RAM memory is stored as a file in drive(cached). Each dataset is stored in this manner.
* Then a read only pointer is pointed to this file and uses it as a substitute for RAM, basicalluy using hard drive as a extenstion of RAM memory.

--> With above features from Datasets, we can now load a large dataset in a machine provided adequate hard disk space is available.

* In addition to zero-shot/zero-overhead Datasets use Apache Arrow under the hood, which makes it very efficient to access any element. Depeneding on speed of hard drive and batch sizer, we can iterate from tenths of GB to serveral GB/s.

#### Streaming

***What if we don't have enough hard disk space to load data with memory mapping?***

This is where streaming comes in.

* We can push our dataset to huggingface_hub and download the data on the fily.
* We'll get a `IterableDataset`
* With IterableDataset, we've to fetch data in order and cannot access elements randomnly
* `shuffle()` will download samples over a buffer of examples
* This will randomize order of files for every iteration.

To upload files to huggingface_hub refer page 692 to 696.

## Building a Tokenizer

* Now we've a dataset, next we've to efficiently process the data to feed it to the model
* *Can we use tokenizers like in previous notebooks?* -- **No** Because the tokenizers have their preprocessing pipeline for a specific domain or dataset. We've to consider the domain and preprocessing of an Tokenizer(from huggingface) before using it.
* Some pitfalls of using a Tokenizer without understading it's inner workings:
    1. If we use T5 tokenizer trained on C4 corputs, we'll never see common English words like "sex." Since it used an extensive stopword filtering to create the dataset.
    2. If we use CamemBERT tokenizer which is trained on French subset of OSCAR corpus, it will be unaware of English words.
* Due to these concers, we've to stick with same preprocessing design choices selected for pretraining. Otherwise the model may be fed out-of-distribution patterns or unkown tokens.

> Training a tokenizer doesn't involve backpropogation or weights like training a model. It's a way to create a optimal mapping of string of text to a list of integers that can be ingested to a model. In today's tokenizers the optimal sstring to integer conversion involves:
    1. A list of atomic strings vocabulary
    2. A method to cut, nromaalize or map a text string into a list of indices with this vocabulary. This is then fed to our neural network.

