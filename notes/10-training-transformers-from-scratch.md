# Training transformers from scratch

* 📝To train large model with billions of paramters, we need special tools for distributed training... Training a model of this capacity requried multiple GPU's'

📝Choosing between training or fine-tuning depends on two things?

1. Corpus size
    * As the amount of training data we have inches closer to amount of data required for pretraing, it becomes an intersting choice to training the model and tokenizer from scratch(provided the compute resources).
2. Difference between corpus and pretrained model corpus
    * ⚠️ Pretrained model forces us to use it's tokenizer. If the tokenizer is trained on a different domain then the results will be poor, as tokenization is the inital step of converting raw text to numbers.

## Large Datasets and Where to Find Them

There are many domains where large amount of data at hand might be available ranging from biomedical datasets to programming codebases. In mose cases, these datasets are unlabeled, and their large size means that they can usually be labeled thriugh use of heeuristics(past labelling experience) or by using accompanying metadata that is stored during the gathering process.

Nevertheless unlaballed or heuristice labelled large corpus is useful. For instance it can be used to fine tune a language model for domain adaptation.

The decision between fine-tuning and training from scratch is dependent on two things:

1. What's the size of fine-tuning corpus?
2. What's the domain differences between pretrained models and the corpus?

### Challenges of Building a Large-Scale Corpus

Quality of a pretrained model depends on the pretrained corpus itself, as the model inherits defects from the corpus. Hence before creating one, let's become aware of the common issues and challenges associated with building a large corpora for pretraining.

```
Pretraining Corpus[good/bad] ---Training---> Pretrained model[good/bad]
```

***1. Can we be aware of what's inside a very large dataset?***

*As the dataset grows larger and larger, the chances of full control or precise idea of what is inside dimishes.*

***2. How is a large dataset created? which might give some information on visiblity of the dataset***

* *It's not created by dedicated people who create one sample at a time, while being aware and knowledgeable of the full pipeline and the task that the machine learning model will be applied to.*
* *It has more chances of creating in an automatic or semiautomatic way by collecting data that is a side effect of some other activites. For example, it may consists of all the documents(contracts, purchase orders etc.) that a company stores, logs from user activites, or data gatherd from internet.*

***3. What are the consequences of creating a corpora with such high degree of automation?***

* *Limited control over the content and te way ther are created, thus increasing the risk of training a model on biased or lower-quality data.*
* *Recent investigations of large-scale datasets like BookCorpus and C4 which were used to train BERT and T4, have uncoverd (among other things) that:*
    * A significant proportion of the C4 corpus is machine-translated rather than by humans.
    * Disparate erasure of African-American English as a result of stopword filtering in C4 resulted in an underrepresentation of such content.
    * It's typically diffult to find a middle ground between including(often too much) sexually or other explicit content and totally ersation all mention of sexuality or gender. As a surprising consequence of this, a rather common word like sex(both neutral and explicit meanings) is completley unknown to tokenizer that is trained on C4, since this word is absent form corpus.

This discrepancies might not be incompatible if the downstream task requries such a skew. For example, In BookCorpus there's a strong overepresentation of romance novels and if a model is intended to be romance novel writing tool this skew is good for this task.

***Some gpt completions to validate the skew above***

*The below text are generated by two different gpt models of same size on same input prompt but different dataset, gpt-2 is trained on web and blogs reddit articles.*

```
GPT completions:
 1.
When they came back, the next one would be the best.
 the first and only person who might pped a gun was david becker.
 becker stared up at the gray beams from the ceiling. he was terrified. the light was blinding. it
2.
When they came back, she was ready. "
 my jaw had dropped. " you've been watching me? "
 " uh - huh, " he smiled. " i found your address on the phone. "
 that made sense. i
3.
When they came back.
 i would find out soon enough. but right now, my mind was busy processing all the information i was learning to deal with at a late stage of the journey in an uncomfortable sort of way. once we were in the
GPT-2 completions:
 1.
When they came back to look over their shoulders they noticed a tiny black bear and she ran away; I thought they'd be worried, though the bear had a very unusual, large, tail with a large sharp sharp-edged claw.
The
2.
When they came back to him at the top of the stairs.

"I want to say this to anyone in the world out there who's afraid of Muslims," Sahlik said. "Just stop telling me what I don't want
3.
When they came back to work, they said, "I can't do anything about that. This is the best that's possible for me, so I'm sure we'll do something for next year." They also had some ideas. "I
```

On looking at these few samples, we can see the romantic skew in GPT generation, which will typically imagine an interaction between a man and a woman. On the other hand GPT-2 generation trained on webtext linked to and from reddit articles and mostly adopts a neutral *they* in it's generationsm whicg contation "blog-like" or adventure related elemets.

In general, any model trained on dataset will reflect the language bias and over-or underrepresentation of populations and events in its training data. These biases in the behaviour of the model are importatnt to take into consideration with reagard to the target audience interacting with the model.

The brief introduction above will give an idea of the difficult challenges faced during creation of a large corpora. With these in mind, let's create a large dataset.
