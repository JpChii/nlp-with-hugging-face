# Transformer Taxonomy

In this respective notebook, below topic'll be covered,
1. Encoder
2. Decoder
3. Encoder-only
4. Decoder-only
5. Encocer-decoder
6. self attention
7. Scaled dot-product attention
8. Mult-headed Attention
9. Layer Normalization
10. Positional embeddings'll be covered
11. Visualizing attentions with BertViz

Most of the notes will be present in notebooks. Titbits will be added here. Please check the [3-transformer-taxonomy.ipynb](../notebooks/3-transformer-taxonomy.ipynb) for more details